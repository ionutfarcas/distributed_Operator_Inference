{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64e1f1d",
   "metadata": {},
   "source": [
    "### This step-by-step tutorial showcases our recently formulated distributed Operator Inference (dOpInf) algorithm in a two-dimensional transient flow past a circular cylinder scenario governed by the 2D incompressible Navier-Stokes equations \n",
    "### The goal of this tutorial is to provide a detailed description of dOpInf and to guide users on practical details and facilitate a seamless integration with complex applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c9249",
   "metadata": {},
   "source": [
    "### Problem description\n",
    "* #### we consider the canonical problem of two-dimensional transient flow past a circular cylinder, a widely-used benchmark in computational fluid dynamics and reduced-order modeling\n",
    "* #### the fluid flow is governed by the 2D incompressible Navier-Stokes equations\n",
    "#### $$ \n",
    "\\begin{align*}\n",
    "\\partial_t \\mathbf{u} + \\nabla \\cdot (\\mathbf{u} \\otimes \\mathbf{u}) & = \\nabla p + Re^{-1}\\Delta \\mathbf{u} \\\\\n",
    "\\nabla \\cdot \\mathbf{u} & = 0,\n",
    "\\end{align*}\n",
    "$$ \n",
    "#### where $p \\in \\mathbb{R}$ denotes the pressure, $\\mathbf{u} = (u_x, u_y)^\\top \\in \\mathbb{R}^2$ denotes the $x$ and $y$ components of the velocity vector, and $Re$ denotes the Reynolds number\n",
    "* #### the problem setup, geometry, and parameterization are based on the [DFG 2D-3 benchmark in the FeatFlow suite](https://wwwold.mathematik.tu-dortmund.de/~featflow/en/benchmarks/cfdbenchmarking/flow/dfg_benchmark2_re100.html)\n",
    "* #### this setup uses $Re = 100$, which represents a value that is above the critical Reynolds number for the onset of the two-dimensional vortex shedding\n",
    "* #### the physical domain is depicted in the figure below:\n",
    "<img src=\"images/navier_stokes_cylinder_geometry.png\" width=\"700\" class=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d004ce0d-543f-42db-ab50-1162dc2bfd88",
   "metadata": {},
   "source": [
    "### Numerical discretization\n",
    "* #### we solve the 2D Navier Stokes equations numerically in double precision using finite elements with the [FEniCS](https://fenicsproject.org/) software package\n",
    "* #### our implementation provided in the [**generate_high_fidelity_data.py**](./high_fidelity_code/generate_high_fidelity_data.py) script closely follows this [tutorial](https://fenicsproject.org/pub/tutorial/html/._ftut1009.html)\n",
    "* #### we use piecewise quadratic finite elements for the velocity and piecewise linear elements for the pressure\n",
    "* #### the resulting system of equations is large and sparse, making direct solvers impractical\n",
    "* #### we employ iterative Krylov subspace methods with preconditioning for improved performance\n",
    "* #### we utilize the biconjugate gradient stabilized (BiCGstab) method and the conjugate gradient (CG) method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6660b5-c886-4b4c-93a5-a460a9fb9863",
   "metadata": {},
   "source": [
    "### Acquiring the training data\n",
    "* #### the finite element mesh contains $n_x = 146,339$ degrees of freedom\n",
    "* #### the discretized governing equations are integrated over the time interval $[0, 10]$ seconds in increments of $\\Delta t = 2.5 \\times 10^{-5}$ seconds\n",
    "* #### this amounts to a total of $40,000$ time instants of the pressure and velocity fields\n",
    "* #### in the model reduction experiments that follow, we simplify the model by omitting the pressure term, which is justified by the fact that, for both transient and periodic flow regimes, the pressure can be implicitly determined from the velocity field through the incompressibility constraint\n",
    "* #### this means that the number of state variables is $n_s = 2$\n",
    "* #### therefore, the size of one data snapshot is $n = n_s \\times n_x = 292,678$\n",
    "* #### the target time horizon is $[4, 10]$ seconds, corresponding to the periodic regime\n",
    "* #### training data are collected over $[4, 7]$ seconds, whereas the remainder of the target time horizon is used for predictions beyond training\n",
    "* #### storing the full velocity fields over the training horizon would require approximately $28$ GB of storage, exceeding the capabilities of a typical personal computer\n",
    "* #### we downsample the training data by a factor of $20$, reducing its storage footprint to a manageable size of around $1.4$ GB for $n_t = 600$ downsampled snapshots\n",
    "* #### the training data were saved to disk in a single HDF5 file [**velocity_training_snapshots.h5**](./navier_stokes_benchmark/velocity_training_snapshots.h5)\n",
    "* #### this file contains two datasets, $u\\_x$ and $u\\_y$, for the two downsampled velocity components of dimension $146,339 \\times 600$\n",
    "* #### this amounts to a snapshot matrix $\\mathbf{S} \\in \\mathbb{R}^{n \\times n_t} = \\mathbb{R}^{292,678 \\times 600}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c36b476",
   "metadata": {},
   "source": [
    "### Steps to implement the dOpInf algorithm\n",
    "* #### let $p \\in \\mathbb{N}_{\\geq 2}$ denote the number of compute cores to perform dOpInf\n",
    "* #### this tutorial focuses on a dOpInf implementation in Python using the Message Passing Interface (MPI) library [mpi4py](https://mpi4py.readthedocs.io/en/stable/)\n",
    "* #### for convenience, we will use in the following the notation $i = 0, 1, \\dots, p-1$ to denote both the $i$th core and its corresponding MPI rank\n",
    "  \n",
    "#### Step I: Parallel training data loading\n",
    "* #### we start by creating a parallel client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cafd2-114b-4a87-ae6d-57e51b3e8c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "rc   = ipp.Client()\n",
    "view = rc[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d945c2-61c0-4901-bbc8-66168c21fe93",
   "metadata": {},
   "source": [
    "* #### we then import the necessary libraries for our computations, define key variables, and initialize the standard MPI communicator\n",
    "* #### the rank of each compute core is given by the variable <em>rank</em> ranging from $0$ to $p-1$, whereas the total size of the communicator ($p$ in our case) is given by the variable <em>size</em>\n",
    "* #### note that each cell that executes parallel code must start with the parallel magic command <em>%%px</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df08a1-9471-4ffc-a092-fb7f0d6bca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c01fb3-05d7-44f9-a3de-482a3ffd39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# DoF setup\n",
    "ns \t= 2\n",
    "n \t= 292678 \n",
    "nx  = int(n/ns)\n",
    "\n",
    "# number of training snapshots\n",
    "nt = 600\n",
    "\n",
    "# state variable names\n",
    "state_variables = ['u_x', 'u_y']\n",
    "\n",
    "# path to the HDF5 file containing the training snapshots\n",
    "H5_training_snapshots = 'navier_stokes_benchmark/velocity_training_snapshots.h5'\n",
    "\n",
    "# number of time instants over the time domain of interest (training + prediction)\n",
    "nt_p = 1200\n",
    "\n",
    "# MPI initialization\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3394a4-2172-43c6-bc93-7a980fca20f5",
   "metadata": {},
   "source": [
    "* #### we then load the training data in parallel\n",
    "* #### the user must choose a strategy for distributing the full training dataset $\\mathbf{S} \\in \\mathbb{{R}}^{n \\times n_t}$ into $p$ non-overlapping blocks $\\mathbf{S}_i \\in \\mathbb{R}^{n_i \\times n_t}$ for $i = 0, 1, \\dots, p-1$ such that $\\sum_{i = 0}^{p-1} n_i = n$\n",
    "* #### the splitting strategy depends on the target parallel architecture and whether training data manipulations are necessary\n",
    "* #### in the code below, we distribute the number of spatial degrees of freedom $n_x$ into $p$ independent components $n_{x, i}$ such that $\\sum_{i=0}^{p-1} n_{x, i} = 146,339$\n",
    "* #### moreover, $n_i = 2 n_{x, i}$ and $\\sum_{i=0}^{p-1} n_{i} = 292,678$\n",
    "* #### this corresponds to decomposing the spatial domain into $p$ non-overlapping subdomains\n",
    "* #### this splitting scheme allows to efficiently perform data transformations in parallel\n",
    "* #### each rank then reads the two velocity components corresponding to its subdomain into a snapshot matrix denoted by <em>Q\\_rank</em> of size $2 n_{x, i} \\times n_t = n_{i} \\times n_t$; we explain below why we denote the snapshot matrix on each rank by <em>Q\\_rank</em> instead of <em>S\\_rank</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd2b71-5db5-4985-a928-f1b6cb4ce632",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def distribute_nx(rank, nx, size):\n",
    "\t\"\"\"\n",
    " \tdistribute_nx distributes the spatial DoF nx into chunks of size nx_i such that \n",
    " \t\\sum_{i=0}^{p-1} nx_i = nx where p is the number of used compute cores\n",
    "\n",
    " \t:rank: \tthe MPI rank i = 0, 1, ... , p-1 that will execute this function\n",
    " \t:n_x: \tnumber of DoF used for spatial discretization\n",
    " \t:size: \tsize of the MPI communicator (p in our case)\n",
    " \t\n",
    " \t:return: the start and end index of the local DoF, and the number of local DoF for each rank\n",
    " \t\"\"\"\n",
    "\n",
    "\tnx_i_equal = int(nx/size)\n",
    "\n",
    "\tnx_i_start = rank * nx_i_equal\n",
    "\tnx_i_end   = (rank + 1) * nx_i_equal\n",
    "\n",
    "\tif rank == size - 1 and nx_i_end != nx:\n",
    "\t\tnx_i_end += nx - size*nx_i_equal\n",
    "\n",
    "\tnx_i = nx_i_end - nx_i_start\n",
    "\n",
    "\treturn nx_i_start, nx_i_end, nx_i\n",
    "\n",
    "# the start and end indices, and the total number of snapshots for each MPI rank\n",
    "nx_i_start, nx_i_end, nx_i = distribute_nx(rank, nx, size)\n",
    "        \n",
    "# allocate memory for the snapshot data corresponding to each MPI rank\n",
    "# the full snapshot data has been saved to disk in HDF5 format\n",
    "Q_rank = np.zeros((ns * nx_i, nt))\n",
    "with h5.File(H5_training_snapshots, 'r') as file:\n",
    "\n",
    "    for j in range(ns):\n",
    "        Q_rank[j*nx_i : (j + 1)*nx_i, :] = \\\n",
    "\t\t          file[state_variables[j]][nx_i_start : nx_i_end, :]\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea6550-f113-4f1f-b930-ff21f1bd913b",
   "metadata": {},
   "source": [
    "### Step II: Parallel training data manipulations\n",
    "* #### in the next step, we perform any necessary data manipulations, which can vary depending on the specific problem\n",
    "* #### this might include variable transformations for scenarios with non-polynomial governing equations or scaling transformations for those involving multiple states with varying scales\n",
    "* #### if data transformations are used, we denote by $\\mathbf{Q}_i \\in \\mathbb{R}^{m_i \\times n_t}$ the \\emph{transformed} snapshot data on each compute core, with $\\sum_{i=0}^{p-1} m_i = m \\geq n$ denoting the transformed snapshot dimension\n",
    "* #### note that $m$ exceeds $n$ when the number of lifted variables exceeds the number of original state variables\n",
    "* #### if data transformations are not required, we employ $\\mathbf{S}_i$ as is and use the notation $\\mathbf{Q}_i = \\mathbf{S}_i$ and $m_i = n_i$ for convenience\n",
    "* #### for the considered 2D Navier-Stokes example, the only employed data transformation is snapshot centering with respect to the temporal mean over the training time horizon\n",
    "* #### since the data splitting was done over the spatial domain, performing centering with respect to the mean is trivial in an MPI program, as illustrated in the code snippet below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4031a374-d8a1-481f-8555-bcc3e344b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# compute the temporal mean of each variable on each rank\n",
    "temporal_mean_rank  = np.mean(Q_rank, axis=1)\n",
    "# center (in place) each variable with respect to its temporal mean on each rank\n",
    "Q_rank              -= temporal_mean_rank[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ebd198-c51b-41fc-bedf-c50b63c778f1",
   "metadata": {},
   "source": [
    "### Step III: Parallel dimensionality reduction\n",
    "* #### in the next step, we perform parallel dimensionality reduction, which represent the high-dimensional (transformed) snapshot data with global dimension $m$ in a lower-dimensional space of dimension $r$ such that $r \\ll m$\n",
    "* #### here, the reduced space is a linear subspace, spanned by the column vectors forming the $r$-dimensional POD basis\n",
    "* #### this step is typically the most computationally and memory-intensive part of the standard, serial OpInf formulation\n",
    "* #### starting from the method of snapshots, we can scalably and efficiently represent the high-dimensional snapshot data in the low-dimensional subspace spanned by the $r$-dimensional POD basis vectors without explicitly having to compute the POD basis and without introducing approximations by using, for example, a randomized SVD technique\n",
    "* #### we start by computing the Gram matrices $\\mathbf{D}_i = \\mathbf{Q}_i^\\top \\mathbf{Q}_i \\in \\mathbb{R}^{n_t \\times n_t}$ on each core, followed by their summation $\\mathbf{D} = \\sum_{i=1}^p \\mathbf{D}_i$ obtained via a colective parallel reduction\n",
    "* #### consider the thin singular value decomposition (SVD) of $\\mathbf{Q}$ computed at a cost in $\\mathcal{O}(mn_t^2)$:\n",
    "$$ \\begin{equation}\n",
    "    \\mathbf{Q} = \\mathbf{V} \\mathbf{\\Sigma} \\mathbf{W}^\\top,\n",
    "\\end{equation}\n",
    "$$\n",
    "#### where $\\mathbf{V} \\in \\mathbb{R}^{m \\times n_t}$ contains the left singular vectors, $\\mathbf{\\Sigma} \\in \\mathbb{R}^{n_t \\times n_t}$ is a diagonal matrix comprising the singular values in non-decreasing order $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_{n_t}$, where $\\sigma_j$ denotes the $j$th singular value, and $\\mathbf{W} \\in \\mathbb{R}^{n_t \\times n_t}$ contains the right singular vectors\n",
    "* #### the rank-$r$ POD basis $\\mathbf{V}_r \\in \\mathbb{R}^{m \\times r}$ is obtained from the first $r$ columns of $\\mathbf{V}$ corresponding to the $r$ largest singular values\n",
    "* #### in the standard OpInf formulation, the low-dimensional representation of $\\mathbf{Q}$ in the linear subspace spanned by the columns of $\\mathbf{V}_r$ is computed at a cost in $\\mathcal{O}(r  m  n_t)$ as\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{Q}} = \\mathbf{V}_r^\\top \\mathbf{Q} \\in \\mathbb{R}^{r \\times n_t}\n",
    "\\end{equation}\n",
    "$$\n",
    "* #### since the original snapshot data was partitioned non-overlappingly, it follows that\n",
    "$$\n",
    "\\begin{equation*} \n",
    "    \\mathbf{D} = \\sum_{i=1}^p \\mathbf{D}_i = \\sum_{i=1}^p \\mathbf{Q}_i^\\top \\mathbf{Q}_i = \\mathbf{Q}^\\top \\mathbf{Q}.\n",
    "\\end{equation*}\n",
    "$$\n",
    "* #### we moreover have that\n",
    "$$\n",
    "\\begin{equation} \n",
    "    \\mathbf{D} = \\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{W} \\mathbf{\\Sigma} \\mathbf{V}^\\top \\mathbf{V} \\mathbf{\\Sigma} \\mathbf{W}^\\top = \\mathbf{W} \\mathbf{\\Sigma}^2 \\mathbf{W}^\\top \\Rightarrow \\mathbf{D} \\mathbf{W} = \\mathbf{W} \\mathbf{\\Sigma}^2,\n",
    "\\end{equation}\n",
    "$$\n",
    "#### which implies that the eigenvalues of $\\mathbf{D}$ are the squared singular values of $\\mathbf{Q}$, and the eigenvectors of $\\mathbf{D}$ are equivalent (up to a sign change) to the right singular vectors of $\\mathbf{Q}$\n",
    "* #### each core then computes the eigenpairs $\\{(\\lambda_k, \\mathbf{u}_k)\\}_{k=1}^{n_t}$ of $\\mathbf{D}$, where $\\lambda_k$ are the real and non-negative eigenvalues and $\\mathbf{u}_k \\in \\mathbb{R}^{n_t}$ denote the corresponding eigenvectors\n",
    "* #### we also ensure that the eigenpairs are arranged such that $\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_{n_t}$\n",
    "* #### the rank-$r$ POD basis can be also computed as\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbf{V}_r = \\mathbf{Q} \\mathbf{W}_r \\mathbf{\\Sigma}_r^{-1} = \\mathbf{Q} \\mathbf{U}_r \\mathbf{\\Lambda}_r^{-\\frac{1}{2}}, \n",
    "\\end{equation}\n",
    "$$\n",
    "#### where $\\mathbf{U}_r = \\begin{bmatrix} \\mathbf{u}_1 \\vert \\mathbf{u}_2 \\vert \\dots \\vert \\mathbf{u}_r \\end{bmatrix}$ and $\\mathbf{\\Lambda}_r = \\mathrm{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_r)$\n",
    "* #### however, the above expression implies that\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{Q}} = \\mathbf{V}_r^\\top \\mathbf{Q} = \\left(\\mathbf{Q} \\mathbf{U}_r \\mathbf{\\Lambda}_r^{-\\frac{1}{2}}\\right)^\\top \\mathbf{Q} = \\mathbf{T}_r^\\top \\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{T}_r^\\top \\mathbf{D},\n",
    "\\end{equation}\n",
    "$$\n",
    "#### where we used the notation $\\mathbf{T}_r =  \\mathbf{U}_r \\mathbf{\\Lambda}_r^{-\\frac{1}{2}} \\in \\mathbb{R}^{n_t \\times r}$\n",
    "* #### therefore, the representation of the high-dimensional (transformed) snapshots in the low-dimensional linear subspace spanned by the rank-$r$ POD basis vectors can be efficiently computed in terms of two small matrices, $\\mathbf{T}_r$ and $\\mathbf{D}$, without explicitly requiring the POD basis\n",
    "* #### in our implementation, we choose the reduced dimension, $r$, such that the total retained energy corresponding to the first $r$ POD modes is $99.96 \\%$, that is,\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\sum_{k=1}^{r} \\sigma_k^2}{\\sum_{k=1}^{n_t} \\sigma_k^2} \\geq 0.9996\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e2d7d3-269a-462d-966e-d1aa2a67bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# compute the local Gram matrices on each rank\n",
    "D_rank = np.matmul(Q_rank.T, Q_rank)\n",
    "\n",
    "# compute the global Gram matrix via a parallel reduction of the local Gram matrices, and broadcast the result to all ranks\n",
    "D_global = np.zeros_like(D_rank)\n",
    "comm.Allreduce(D_rank, D_global, op=MPI.SUM)\n",
    "\n",
    "# compute the eigendecomposition of the positive, semi-definite global Gram matrix\n",
    "eigs, eigv = np.linalg.eigh(D_global)\n",
    "\n",
    "# order eigenpairs by increasing eigenvalue magnitude\n",
    "sorted_indices  = np.argsort(eigs)[::-1]\n",
    "eigs            = eigs[sorted_indices]\n",
    "eigv            = eigv[:, sorted_indices]\n",
    "\n",
    "# define target retained energy for the dOpInf ROM\n",
    "target_ret_energy = 0.9996\n",
    "\n",
    "# compute retained energy for r bteween 1 and nt\n",
    "ret_energy  = np.cumsum(eigs)/np.sum(eigs)\n",
    "# determine the minimum value of r that exceeds the prescribed retained energy threshold\n",
    "r           = np.argmax(ret_energy > target_ret_energy) + 1\n",
    "\n",
    "# compute the auxiliary Tr matrix\n",
    "Tr_global \t= np.matmul(eigv[:, :r], np.diag(eigs[:r]**(-0.5)))\n",
    "# compute the low-dimensional representation of the high-dimensional transformed snapshot data\n",
    "Qhat_global = np.matmul(Tr_global.T, D_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a9a68-d108-43df-8fef-5391f7740606",
   "metadata": {},
   "source": [
    "### Step IV: Parallel reduced operator learning via Operator Inference\n",
    "* #### the Navier-Stokes equations have only linear and quadratic terms in the governing equations\n",
    "* #### we additionally have a constant term introduced into the ROM, due to centering\n",
    "* #### since the training dataset was severly downsampled in time, we employ the time-discrete formulation of OpInf\n",
    "* #### the goal is to determine the reduced operators $\\hat{\\mathbf{c}} \\in \\mathbb{R}^{r}, \\hat{\\mathbf{A}} \\in \\mathbb{R}^{r\\times r}$, and $\\hat{\\mathbf{H}} \\in \\mathbb{R}^{r\\times r^2}$ defining the discrete quadratic ROM\n",
    "$$\n",
    "\\begin{equation} \n",
    "    \\hat{\\mathbf{q}}[k + 1] = \\hat{\\mathbf{A}}\\hat{\\mathbf{q}}[k] + \\hat{\\mathbf{H}}\\left(\\hat{\\mathbf{q}}[k] \\otimes \\hat{\\mathbf{q}}[k] \\right) + \\hat{\\mathbf{c}}\n",
    "\\end{equation}\n",
    "$$\n",
    "#### that best match the projected snapshot data in a minimum residual sense by solving the linear least-squares minimization\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathop{\\mathrm{argmin}}_{\\hat{\\mathbf{O}}} \\left\\lVert \\hat{\\mathbf{D}}\\hat{\\mathbf{O}}^{\\top} - \\hat{\\mathbf{Q}}_2^\\top \\right\\rVert_F^2 + \\beta_{1} \\left(\\left\\lVert\\hat{\\mathbf{A}}\\right\\rVert_F^2 + \\left\\lVert\\hat{\\mathbf{c}}\\right\\rVert_F^2\\right) + \\beta_{2} \\left\\lVert\\hat{\\mathbf{H}}\\right\\rVert_F^2,\n",
    "\\end{equation}\n",
    "$$\n",
    "#### where  $\\hat{\\mathbf{O}} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{\\mathbf{A}} \\, \\vert \\, \\hat{\\mathbf{H}} \\, \\vert \\, \\hat{\\mathbf{c}}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{r \\times (r + r^2 + 1)}$ denotes the unknown operators and  $\\hat{\\mathbf{D}} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{\\mathbf{Q}}_1^\\top \\, \\vert \\, \\hat{\\mathbf{Q}}_1^\\top \\otimes \\hat{\\mathbf{Q}}_1^\\top \\, \\vert \\, \\hat{\\mathbf{1}}_{n_t - 1}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{(n_t - 1) \\times (r + r^2 +1)}$ the OpInf data, $F$ denotes the Frobenius norm, and\n",
    "\\begin{equation} \n",
    "    \\hat{\\mathbf{Q}}_{1} =\n",
    "     \\begin{bmatrix}\n",
    "\\vert & \\vert & & \\vert\\\\\n",
    "     \\hat{\\mathbf{q}}_1 &\n",
    "     \\hat{\\mathbf{q}}_2 &\n",
    "     \\ldots &\n",
    "     \\hat{\\mathbf{q}}_{n_t - 1}\\\\\n",
    "     \\vert & \\vert & & \\vert\n",
    "     \\end{bmatrix} \\in \\mathbb{R}^{r \\times n_t-1} \\quad \\text{and} \\quad \n",
    "     \\hat{\\mathbf{Q}}_{2} =\n",
    "     \\begin{bmatrix}\n",
    "\\vert & \\vert & & \\vert\\\\\n",
    "     \\hat{\\mathbf{q}}_2 &\n",
    "     \\hat{\\mathbf{q}}_3 &\n",
    "     \\ldots &\n",
    "     \\hat{\\mathbf{q}}_{n_t}\\\\\n",
    "     \\vert & \\vert & & \\vert\n",
    "     \\end{bmatrix} \\in \\mathbb{R}^{r \\times n_t-1}\n",
    "\\end{equation}\n",
    "* #### to address overfitting and other sources of error, we introduce regularization hyperparameters $\\beta_{1}, \\beta_{2} \\in \\mathbb{R}$\n",
    "* #### we start with providing the code for four auxiliary functions that we will need later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dab116-6c29-46f4-a1b9-f1b2c18c16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def distribute_reg_pairs(rank, n_reg, size):\n",
    "\t\"\"\"\n",
    " \tget_reg_params_per_rank returns the index of the first and last regularization pair for each MPI rank\n",
    " \n",
    " \t:rank:    the MPI rank i = 0, 1, ... , p-1 that will execute this function\n",
    " \t:n_reg:   total number of regularization parameter pairs\n",
    " \t:size: \t  size of the MPI communicator (p in our case)\n",
    "\n",
    " \t:return: the start and end indices of the regularization pairs for each MPI rank\n",
    " \t\"\"\"\n",
    "\n",
    "\tnreg_i_equal = int(n_reg/size)\n",
    "\n",
    "\tstart = rank * nreg_i_equal\n",
    "\tend   = (rank + 1) * nreg_i_equal\n",
    "\n",
    "\tif rank == size - 1 and end != n_reg:\n",
    "\t\tend += n_reg - size*nreg_i_equal\n",
    "\n",
    "\treturn start, end\n",
    "\n",
    "def compute_Qhat_sq(Qhat):\n",
    "\t\"\"\"\n",
    "\tcompute_Qhat_sq returns the non-redundant terms in Qhat \\otimes Qhat\n",
    "\n",
    "\t:Qhat: reduced snapshot data\n",
    "\n",
    "\t:return: the non-redundant in Qhat \\otimes Qhat\n",
    "\t\"\"\"\n",
    "\n",
    "\tif len(np.shape(Qhat)) == 1:\n",
    "\n",
    "\t    r \t\t= np.size(Qhat)\n",
    "\t    prods \t= []\n",
    "\t    for i in range(r):\n",
    "\t        temp = Qhat[i]*Qhat[i:]\n",
    "\t        prods.append(temp)\n",
    "\n",
    "\t    Qhat_sq = np.concatenate(tuple(prods))\n",
    "\n",
    "\telif len(np.shape(Qhat)) == 2:\n",
    "\t    K, r \t= np.shape(Qhat)    \n",
    "\t    prods \t= []\n",
    "\t    \n",
    "\t    for i in range(r):\n",
    "\t        temp = np.transpose(np.broadcast_to(Qhat[:, i], (r - i, K)))*Qhat[:, i:]\n",
    "\t        prods.append(temp)\n",
    "\t    \n",
    "\t    Qhat_sq = np.concatenate(tuple(prods), axis=1)\n",
    "\n",
    "\telse:\n",
    "\t    print('invalid input!')\n",
    "\n",
    "\treturn Qhat_sq\n",
    "\n",
    "def compute_train_err(Qhat_train, Qtilde_train):\n",
    "\t\"\"\"\n",
    "\tcompute_train_err computes the OpInf training error\n",
    "\n",
    "\t:Qhat_train:   reference data computed by projecting the high-dimensional transformed snapshots\n",
    "\t:Qtilde_train: approximate data computed by the dOpInf ROM\n",
    "\n",
    "\t:return: the value of the training error\n",
    "\t\"\"\"\n",
    "\ttrain_err = \\\n",
    "    np.max(np.sqrt(np.sum( (Qtilde_train - Qhat_train)**2, axis=1) / np.sum(Qhat_train**2, axis=1)))\n",
    "\n",
    "\treturn train_err\n",
    "\n",
    "def solve_discrete_dOpInf_model(qhat0, n_steps_trial, dOpInf_red_model):\n",
    "\t\"\"\"\n",
    "\tsolve_discrete_dOpInf_model solves the discrete dOpInf ROM for n_steps_trial over a prescribed trial time horizon\n",
    "\n",
    "\t:qhat0:            reduced initial condition\n",
    "\t:n_steps_trial:    number of time steps for OpInf solution\n",
    "\t:dOpInf_red_model: callback to the dOpInf ROM\n",
    "\n",
    "\t:return: a flag indicating NaN presence in the reduced solution and the reduced solution \n",
    "\t\"\"\"\n",
    "\n",
    "\tQtilde    \t    = np.zeros((np.size(qhat0), n_steps_trial))\n",
    "\tcontains_nans   = False\n",
    "\n",
    "\tQtilde[:, 0] = qhat0\n",
    "\tfor i in range(n_steps_trial - 1):\n",
    "\t    Qtilde[:, i + 1] = dOpInf_red_model(Qtilde[:, i])\n",
    "\n",
    "\tif np.any(np.isnan(Qtilde)):\n",
    "\t    contains_nans = True\n",
    "            \n",
    "\treturn contains_nans, Qtilde.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d1f8f-cf11-4187-9020-2b396b5721c8",
   "metadata": {},
   "source": [
    "* #### we employ a grid search to find the optimal hyperparameter values\n",
    "* #### this involves exploring a range of candidate values for both $\\beta_{1}$ and $\\beta_{2} \\in \\mathbb{R}$ in a nested loop\n",
    "* #### we prescribe discrete sets of candidate values $\\mathcal{B}_1$ and $\\mathcal{B}_2$ for the regularization parameters\n",
    "* #### we consider eight candidate values for both $\\mathcal{B}_1$ and $\\mathcal{B}_2$, noting that in problems with more complex dynamics, sets with larger cardinalities (and bounds) might be necessary to ensure that the optimal pair leads to accurately inferred reduced operators\n",
    "* #### we also prescribe a tolerance for the maximum growth of the inferred reduced coefficients over the trial time horizon, which will be used to determine the optimal regularization pair\n",
    "* #### since the candidate hyperparameters are independent, this search is embarrassingly parallel\n",
    "* #### the optimal hyperparameters are chosen to minimize the training error, subject to the constraint that the inferred reduced coefficient have bounded growth over a trial time horizon $[t_{\\mathrm{init}}, t_{\\mathrm{trial}}]$ with $t_{\\mathrm{trial}} \\geq t_{\\mathrm{final}}$; in our implementation, the trial time horizon is the same as the target horizon, that is, $[4, 10]$ seconds\n",
    "* #### in our implementation, the solution to the OpInf least-squares minimization is determined by solving the normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f908289-f9a8-439d-841a-70f18980e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# import function to compute the Cartesian product of all candidate regularization pairs\n",
    "from itertools import product\n",
    "\n",
    "# ranges for the regularization parameter pairs\n",
    "B1 = np.logspace(-10., 0., num=8)\n",
    "B2 = np.logspace(-4., 4., num=8)\n",
    "\n",
    "# threshold for the maximum growth of the inferred reduced coefficients, used for selecting the optimal regularization parameter pair \n",
    "max_growth = 1.2\n",
    "\n",
    "# compute the Cartesian product of all regularization pairs (beta1, beta2 )\n",
    "reg_pairs_global    = list(product(B1, B2))\n",
    "n_reg_global        = len(reg_pairs_global)\n",
    "\n",
    "# distribute the regularization pairs among the p MPI ranks\n",
    "start_ind_reg_params, end_ind_reg_params    = \\\n",
    "                        distribute_reg_pairs(rank, n_reg_global, size)\n",
    "reg_pairs_rank                              = \\\n",
    "                        reg_pairs_global[start_ind_reg_params : end_ind_reg_params]\n",
    " \n",
    "# extract left and right shifted reduced data matrices for the discrete OpInf learning problem\n",
    "Qhat_1 = Qhat_global.T[:-1, :]\n",
    "Qhat_2 = Qhat_global.T[1:, :]\n",
    "\n",
    "# column dimension of the reduced quadratic operator\n",
    "s = int(r*(r + 1)/2)\n",
    "# total column dimension of the data matrix Dhat used in the discrete OpInf learning problem\n",
    "d = r + s + 1\n",
    "\n",
    "# compute the non-redundant quadratic terms of Qhat_1 \\otimes Qhat_1\n",
    "Qhat_1_sq = compute_Qhat_sq(Qhat_1)\n",
    "\n",
    "# define the constant part (due to mean shifting) in the discrete OpInf learning problem\n",
    "K \t = Qhat_1.shape[0]\n",
    "Ehat = np.ones((K, 1))\n",
    "\n",
    "# assemble the data matrix Dhat for the discrete OpInf learning problem\n",
    "Dhat   = np.concatenate((Qhat_1, Qhat_1_sq, Ehat), axis=1)\n",
    "# compute Dhat.T @ Dhat for the normal equations to solve the OpInf least squares minimization\n",
    "Dhat_2 = Dhat.T @ Dhat\n",
    "\n",
    "# compute the temporal mean and maximum deviation of the reduced training data\n",
    "mean_Qhat_train     = np.mean(Qhat_global.T, axis=0)\n",
    "max_diff_Qhat_train = np.max(np.abs(Qhat_global.T - mean_Qhat_train), axis=0)\n",
    "\n",
    "# dictionary to store the regularization pair for each regularization pair\n",
    "opt_train_err_reg_pair      = {}\n",
    "# dictionary to store the reduced solutions over the target time horizon for each pair\n",
    "Qtilde_dOpInf_reg_pair      = {}\n",
    "# dictionary to store the OpInf ROM CPU time for each regularization pair\n",
    "dOpInf_ROM_rtime_reg_pair   = {}\n",
    "\n",
    "# loop over the regularization pairs corresponding to each MPI rank\n",
    "for pair in reg_pairs_rank:\n",
    "\n",
    "    # extract beta1 and beta2 from each candidate regularization pair\n",
    "    beta1 = pair[0]\n",
    "    beta2 = pair[1]\n",
    "\n",
    "    # regularize the linear and constant reduced operators using beta1, and the reduced quadratic operator using beta2\n",
    "    regg            = np.zeros(d)\n",
    "    regg[:r]        = beta1\n",
    "    regg[r : r + s] = beta2\n",
    "    regg[r + s:]    = beta1\n",
    "    regularizer     = np.diag(regg)\n",
    "    Dhat_2_reg      = Dhat_2 + regularizer\n",
    "\n",
    "    # solve the OpInf learning problem by solving the regularized normal equations\n",
    "    Ohat = np.linalg.solve(Dhat_2_reg, np.dot(Dhat.T, Qhat_2)).T\n",
    "\n",
    "    # extract the linear, quadratic, and constant reduced model operators\n",
    "    Ahat = Ohat[:, :r]\n",
    "    Fhat = Ohat[:, r:r + s]\n",
    "    chat = Ohat[:, r + s]\n",
    "\n",
    "    # define the discrete dOpInf reduced model \n",
    "    dOpInf_red_model    = lambda x: Ahat @ x + Fhat @ compute_Qhat_sq(x) + chat\n",
    "    # extract the reduced initial condition from Qhat_1\n",
    "    qhat0               = Qhat_1[0, :]\n",
    "    \n",
    "    # compute the reduced solution over the trial time horizon, which here is the same as the target time horizon\n",
    "    start_time_dOpInf_eval          = MPI.Wtime()\n",
    "    contains_nans, Qtilde_dOpInf    = solve_discrete_dOpInf_model(qhat0, nt_p, dOpInf_red_model)\n",
    "    end_time_dOpInf_eval            = MPI.Wtime()\n",
    "\n",
    "    time_dOpInf_eval = end_time_dOpInf_eval - start_time_dOpInf_eval\n",
    "\n",
    "    # for each candidate regularization pair, we compute the training error \n",
    "    # we also save the corresponding reduced solution and ROM evaluation time\n",
    "    # and compute the ratio of maximum coefficient growth in the trial period to that in the training period\n",
    "    opt_train_err                               = 1e20\n",
    "    opt_train_err_reg_pair[opt_train_err]       = pair\n",
    "    Qtilde_dOpInf_reg_pair[opt_train_err]       = Qtilde_dOpInf\n",
    "    dOpInf_ROM_rtime_reg_pair[opt_train_err]    = time_dOpInf_eval\n",
    "\n",
    "    if contains_nans == False:\n",
    "        train_err               = compute_train_err(Qhat_global.T[:nt, :], Qtilde_dOpInf[:nt, :])\n",
    "        max_diff_Qhat_trial     = np.max(np.abs(Qtilde_dOpInf - mean_Qhat_train), axis=0)\t\t\n",
    "        max_growth_trial        = np.max(max_diff_Qhat_trial)/np.max(max_diff_Qhat_train)\n",
    "\n",
    "        if max_growth_trial < max_growth:\n",
    "            opt_train_err                               = train_err\n",
    "            opt_train_err_reg_pair[opt_train_err]       = pair\n",
    "            Qtilde_dOpInf_reg_pair[opt_train_err]       = Qtilde_dOpInf\n",
    "            dOpInf_ROM_rtime_reg_pair[opt_train_err]    = time_dOpInf_eval\n",
    "\n",
    "# find the globally minimum training error by reducing local results on each rank, subject to the bound constraint on the inferred reduced coefficients over the prescribed trial time horizon\n",
    "opt_key_rank    = np.min(list(opt_train_err_reg_pair.keys()))\n",
    "opt_key_rank    = np.array([opt_key_rank])\n",
    "opt_key_global  = np.zeros_like(opt_key_rank)\n",
    "\n",
    "comm.Allreduce(opt_key_rank, opt_key_global, op=MPI.MIN)\n",
    "opt_key_global = opt_key_global[0]\n",
    "\n",
    "# extract the optimal regularization pair, and the corresponding reduced solution and dOpInf ROM CPU time\n",
    "if opt_key_rank == opt_key_global:\n",
    "    rank_reg_opt        = rank\n",
    "    Qtilde_dOpInf_opt   = Qtilde_dOpInf_reg_pair[opt_key_global]\n",
    "\n",
    "    reg_pair_opt = opt_train_err_reg_pair[opt_key_global]\n",
    "\n",
    "    beta1_opt = reg_pair_opt[0]\n",
    "    beta2_opt = reg_pair_opt[1]\n",
    "\n",
    "    print('\\033[1m The reduced dimension that satisfies the target retained energy is: {} \\033[0m'.format(r))\n",
    "    print('\\033[1m The optimal regularization pair was found on rank: {} \\033[0m'.format(rank_reg_opt))\n",
    "    print('\\033[1m The optimal regularization pair is: ({}, {}) \\033[0m'.format(beta1_opt, beta2_opt))\n",
    "\n",
    "    dOpInf_ROM_rtime_opt = dOpInf_ROM_rtime_reg_pair[opt_key_global]\n",
    "else:\n",
    "    rank_reg_opt        = -1\n",
    "    Qtilde_dOpInf_opt   = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c70840-b0f1-4dfe-afc8-e03f431e78d3",
   "metadata": {},
   "source": [
    "### Step V: Parallel postprocessing of the reduced solution\n",
    "* #### in the final step, we show how to postprocess the obtained reduced solution in parallel\n",
    "* #### the code below demonstrates how to use the reduced solution determined using the optimal regularization pair to compute the two approximate velocity components in the original coordinates at three probe locations positioned near the mid-channel, with increasing distance from the circular cylinder, namely $(0.40, 0.20), (0.60, 0.20)$, and $(1.00, 0.20)$\n",
    "* #### the corresponding grid point indices within each snapshot for these locations are $\\{16,992; 48,250; 130,722\\}$\n",
    "* #### let $\\tilde{\\mathbf{Q}} \\in \\mathbb{R}^{r \\times 1,200}$ denote the matrix containing the reduced solution\n",
    "* #### we first broadcast $\\tilde{\\mathbf{Q}}$ from the rank holding the optimal regularization pair to all other ranks\n",
    "* #### we then map the global probe indices to local indices to identify which rank contains the respective solutions, compute the corresponding components of the local rank-$r$ POD bases, map the reduced solution for each probe to the original coordinates, and save the approximate solutions to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad783ae-64b8-4bb0-8695-f0188f7113a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "target_probe_indices = [48250, 77502, 130722]\n",
    "\n",
    "# broadcast the dOpInf reduced solution from the rank having the optimal regularization pair to all ranks\n",
    "if rank == rank_reg_opt:\n",
    "    for i in range(size):\n",
    "        if i != rank_reg_opt:\n",
    "            comm.send(Qtilde_dOpInf_opt, dest=i)\n",
    "else:\n",
    "    Qtilde_dOpInf_opt = comm.recv(Qtilde_dOpInf_opt, source=rank_reg_opt)\n",
    "\n",
    "# extract and save to disk the approximate solutions at the probe locations with indices specified in target_probe_indices\n",
    "for target_var_index in range(ns):\n",
    "    for j, probe_index in enumerate(target_probe_indices):\n",
    "\n",
    "        # map the global probe indices to local indices\n",
    "        if probe_index >= nx_i_start and probe_index < nx_i_end:\n",
    "            probe_index -= nx_i_start\t\t\t\n",
    "\n",
    "            # compute the components of the POD basis vectors corresponding to the target probe locations on each rank\n",
    "            Phir_probe \t\t\t= np.matmul(Q_rank[probe_index + target_var_index*nx_i, :], Tr_global)\n",
    "             # same for the temporal mean used for centering\n",
    "            temporal_mean_probe = temporal_mean_rank[probe_index + target_var_index*nx_i]\n",
    "\n",
    "            var_probe_prediction = Phir_probe @ Qtilde_dOpInf_opt.T + temporal_mean_probe\n",
    "\n",
    "            np.save('postprocessing/dOpInf_postprocessing/dOpInf_probe_' + str(j + 1) + '_var_' + str(target_var_index + 1) + '.npy', var_probe_prediction)\n",
    "\n",
    "print(\"\\033[1m Rank {} reached the end of the program \\033[0m\".format(rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713863c-7e4d-4342-b3fd-82c09fdc4879",
   "metadata": {},
   "source": [
    "* #### in the final step, we plot the results\n",
    "* #### the code cell below plots the POD singular values and corresponding retained energy computed with our parallel program\n",
    "* #### note that the cell does not start with the parallel magic command <em>%%px</em> which means that it is executed in serial\n",
    "* #### this is because we want only one rank to perform the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ec34c-5c74-476e-8edb-4a0d12b96d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import *\n",
    "import matplotlib as mpl \n",
    "from matplotlib.lines import Line2D\t\n",
    "mpl.use('TkAgg')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "svals = np.load('postprocessing/dOpInf_postprocessing/Sigma_sq_global.npy')\n",
    "\n",
    "no_kept_svals_global \t= 300\n",
    "no_kept_svals_energy \t= 30\n",
    "no_svals_global \t\t= range(1, no_kept_svals_global + 1)\n",
    "no_svals_energy \t\t= range(1, no_kept_svals_energy + 1)\n",
    "\n",
    "retained_energy   = np.cumsum(svals)/np.sum(svals)\n",
    "target_ret_energy = 0.9996\n",
    "\n",
    "r          = np.argmax(retained_energy > target_ret_energy) + 1\n",
    "ret_energy = retained_energy[r]\n",
    "\n",
    "rcParams['lines.linewidth'] = 0\n",
    "rc(\"figure\", dpi=400)   \n",
    "rc(\"font\", family=\"serif\")\n",
    "rc(\"legend\", edgecolor='none')\n",
    "rcParams[\"figure.figsize\"] = (6, 2)\n",
    "rcParams.update({'font.size': 5})\n",
    "\n",
    "charcoal    = [0.1, 0.1, 0.1]\n",
    "color1      = '#D55E00'\n",
    "color2      = '#0072B2'\n",
    "\n",
    "fig \t\t= figure()\n",
    "ax1 \t\t= fig.add_subplot(121)\n",
    "ax2 \t\t= fig.add_subplot(122)\n",
    "\n",
    "rc(\"figure\",facecolor='w')\n",
    "rc(\"axes\",facecolor='w',edgecolor='k',labelcolor='k')\n",
    "rc(\"savefig\",facecolor='w')\n",
    "rc(\"text\",color='k')\n",
    "rc(\"xtick\",color='k')\n",
    "rc(\"ytick\",color='k')\n",
    "\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.yaxis.set_ticks_position('left')\n",
    "ax1.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.yaxis.set_ticks_position('left')\n",
    "ax2.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "## plot\n",
    "ax1.semilogy(no_svals_global, np.sqrt(svals)[:no_kept_svals_global]/np.sqrt(svals[0]), linestyle='-', lw=0.75, color=color1)\n",
    "ax1.set_xlabel('index')\n",
    "ax1.set_ylabel('singular values transformed data')\n",
    "\n",
    "ax2.plot(no_svals_energy, retained_energy[:no_kept_svals_energy], linestyle='-', lw=0.75, color=color1)\n",
    "ax2.set_xlabel('reduced dimension')\n",
    "ax2.set_ylabel('% energy retained')\t\n",
    "ax2.plot([r, r], [0, retained_energy[r]], linestyle='--', lw=0.5, color=charcoal)\n",
    "ax2.plot([0, r], [retained_energy[r], retained_energy[r]], linestyle='--', lw=0.5, color=charcoal)\n",
    "##\n",
    "\n",
    "## \n",
    "xlim = ax1.get_xlim()\n",
    "ax1.set_ylim([1e-10, 1.02e0])\n",
    "\n",
    "x_pos_all \t= np.array([0, 99, 199, 299])\n",
    "labels \t\t= np.array([1, 100, 200, 300])\n",
    "ax1.set_xticks(x_pos_all)\n",
    "ax1.set_xticklabels(labels)\n",
    "\n",
    "ax2.set_xlim([0, 30])\n",
    "ax2.set_ylim([0.4, 1.001])\n",
    "\n",
    "ax2.set_xticks([1, r, 20, 30])\n",
    "ax2.set_yticks([0.5, 0.75, ret_energy , 1])\n",
    "ax2.set_yticklabels([r'$50\\%$', r'$75\\%$', r'$99.98\\%$', ''])\n",
    "###\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355729f-1b73-4046-8f47-fa1ddaae2c10",
   "metadata": {},
   "source": [
    "* #### lastly, we plot the ROM approximate solutions at the three probe locations, $(0.40, 0.20), (0.60, 0.20)$, and $(1.00, 0.20)$\n",
    "* #### the hashed areas on the left mark the training time horizon\n",
    "* #### the approximate solutions are very accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2140ea-a520-4cb0-9554-2f9744a9d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = 4\n",
    "t_end \t= 10\n",
    "t_train = 7\n",
    "dt \t\t= 5e-3 \n",
    "t       = np.arange(t_start, t_end, dt)\n",
    "\n",
    "ref_data_var1_loc1 = np.load('postprocessing/ref_data/ref_probe_1_var_1.npy')\n",
    "ref_data_var1_loc2 = np.load('postprocessing/ref_data/ref_probe_2_var_1.npy')\n",
    "ref_data_var1_loc3 = np.load('postprocessing/ref_data/ref_probe_3_var_1.npy')\n",
    "\n",
    "ref_data_var2_loc1 = np.load('postprocessing/ref_data/ref_probe_1_var_2.npy')\n",
    "ref_data_var2_loc2 = np.load('postprocessing/ref_data/ref_probe_2_var_2.npy')\n",
    "ref_data_var2_loc3 = np.load('postprocessing/ref_data/ref_probe_3_var_2.npy')\n",
    "\n",
    "dOpInf_data_var1_loc1 = np.load('postprocessing/dOpInf_postprocessing/dOpInf_probe_1_var_1.npy')\n",
    "dOpInf_data_var1_loc2 = np.load('postprocessing/dOpInf_postprocessing/dOpInf_probe_2_var_1.npy')\n",
    "dOpInf_data_var1_loc3 = np.load('postprocessing/dOpInf_postprocessing/dOpInf_probe_3_var_1.npy')\n",
    "\n",
    "dOpInf_data_var2_loc1 = np.load('postprocessing/dOpInf_postprocessing/dOpInf_probe_1_var_2.npy')\n",
    "dOpInf_data_var2_loc2 = np.load('postprocessing/dOpInf_postprocessing/dOpInf_probe_2_var_2.npy')\n",
    "dOpInf_data_var2_loc3 = np.load('postprocessing/dOpInf_postprocessing/dOpInf_probe_3_var_2.npy')\n",
    "\n",
    "rcParams['lines.linewidth'] = 0\n",
    "rc(\"figure\", dpi=400)           \n",
    "rc(\"font\", family=\"serif\")      \n",
    "rc(\"legend\", edgecolor='none')\n",
    "rcParams[\"figure.figsize\"] = (8, 4)\n",
    "rcParams.update({'font.size': 6})\n",
    "\n",
    "charcoal    = [0.1, 0.1, 0.1]\n",
    "color1      = '#D55E00'\n",
    "color2      = '#0072B2'\n",
    "\n",
    "fig \t\t= figure()\n",
    "ax11 \t\t= fig.add_subplot(231)\n",
    "ax12 \t\t= fig.add_subplot(232, sharex=ax11, sharey=ax11)\n",
    "ax13 \t\t= fig.add_subplot(233, sharex=ax11, sharey=ax11)\n",
    "ax21 \t\t= fig.add_subplot(234)\n",
    "ax22 \t\t= fig.add_subplot(235, sharex=ax21, sharey=ax21)\n",
    "ax23 \t\t= fig.add_subplot(236, sharex=ax21, sharey=ax21)\n",
    "\n",
    "rc(\"figure\",facecolor='w')\n",
    "rc(\"axes\",facecolor='w',edgecolor='k',labelcolor='k')\n",
    "rc(\"savefig\",facecolor='w')\n",
    "rc(\"text\",color='k')\n",
    "rc(\"xtick\",color='k')\n",
    "rc(\"ytick\",color='k')\n",
    "\n",
    "ax11.spines['right'].set_visible(False)\n",
    "ax11.spines['top'].set_visible(False)\n",
    "ax11.yaxis.set_ticks_position('left')\n",
    "ax11.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "ax12.spines['right'].set_visible(False)\n",
    "ax12.spines['top'].set_visible(False)\n",
    "ax12.yaxis.set_ticks_position('left')\n",
    "ax12.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "ax21.spines['right'].set_visible(False)\n",
    "ax21.spines['top'].set_visible(False)\n",
    "ax21.yaxis.set_ticks_position('left')\n",
    "ax21.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "ax22.spines['right'].set_visible(False)\n",
    "ax22.spines['top'].set_visible(False)\n",
    "ax22.yaxis.set_ticks_position('left')\n",
    "ax22.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "ax13.spines['right'].set_visible(False)\n",
    "ax13.spines['top'].set_visible(False)\n",
    "ax13.yaxis.set_ticks_position('left')\n",
    "ax13.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "ax23.spines['right'].set_visible(False)\n",
    "ax23.spines['top'].set_visible(False)\n",
    "ax23.yaxis.set_ticks_position('left')\n",
    "ax23.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "## plot\n",
    "ax11.plot(t, ref_data_var1_loc1, linestyle='-', lw=1.00, color=charcoal)\n",
    "ax11.plot(t, dOpInf_data_var1_loc1, linestyle='--', lw=1.00, color=color1)\n",
    "\n",
    "ax12.plot(t, ref_data_var1_loc2, linestyle='-', lw=1.00, color=charcoal)\n",
    "ax12.plot(t, dOpInf_data_var1_loc2, linestyle='--', lw=1.00, color=color1)\n",
    "\n",
    "ax13.plot(t, ref_data_var1_loc3, linestyle='-', lw=1.00, color=charcoal)\n",
    "ax13.plot(t, dOpInf_data_var1_loc3, linestyle='--', lw=1.00, color=color1)\n",
    "\n",
    "\n",
    "ax21.plot(t, ref_data_var2_loc1, linestyle='-', lw=1.00, color=charcoal)\n",
    "ax21.plot(t, dOpInf_data_var2_loc1, linestyle='--', lw=1.00, color=color1)\n",
    "\n",
    "ax22.plot(t, ref_data_var2_loc2, linestyle='-', lw=1.00, color=charcoal)\n",
    "ax22.plot(t, dOpInf_data_var2_loc2, linestyle='--', lw=1.00, color=color1)\n",
    "\n",
    "ax23.plot(t, ref_data_var2_loc3, linestyle='-', lw=1.00, color=charcoal)\n",
    "ax23.plot(t, dOpInf_data_var2_loc3, linestyle='--', lw=1.00, color=color1)\n",
    "##\n",
    "\n",
    "ax11.axvline(x=t_train, lw=1.00, linestyle='--', color='gray')\n",
    "ax12.axvline(x=t_train, lw=1.00, linestyle='--', color='gray')\n",
    "ax13.axvline(x=t_train, lw=1.00, linestyle='--', color='gray')\n",
    "ax21.axvline(x=t_train, lw=1.00, linestyle='--', color='gray')\n",
    "ax22.axvline(x=t_train, lw=1.00, linestyle='--', color='gray')\n",
    "ax23.axvline(x=t_train, lw=1.00, linestyle='--', color='gray')\n",
    "\n",
    "ax11.set_title('probe 1 (0.4, 0.2)')\n",
    "ax12.set_title('probe 2 (0.6, 0.2)')\n",
    "ax13.set_title('probe 3 (1.0, 0.2)')\n",
    "\n",
    "ax11.set_ylabel(r'$u_x$')\n",
    "ax21.set_ylabel(r'$u_y$')\n",
    "\n",
    "fig.supxlabel('target time horizon (seconds)')\n",
    "\n",
    "xlim = ax11.get_xlim()\n",
    "ax11.set_xlim([xlim[0], 10])\n",
    "ax21.set_xlim([xlim[0], 10])\n",
    "\n",
    "ylim = ax11.get_ylim()\n",
    "ax11.set_ylim([ylim[0], 1.25])\n",
    "\n",
    "ylim = ax21.get_ylim()\n",
    "ax21.set_ylim([ylim[0], 0.4])\n",
    "\n",
    "rect = Rectangle((0, 0), width=t_train, height=1.25, hatch='/', color='grey', alpha=0.2, label='training region')\n",
    "ax11.add_patch(rect)\n",
    "rect = Rectangle((0, 0), width=t_train, height=1.25, hatch='/', color='grey', alpha=0.2, label='training region')\n",
    "ax12.add_patch(rect)\n",
    "rect = Rectangle((0, 0), width=t_train, height=1.25, hatch='/', color='grey', alpha=0.2, label='training region')\n",
    "ax13.add_patch(rect)\n",
    "\n",
    "rect = Rectangle((0, -0.4), width=t_train, height=0.8, hatch='/', color='grey', alpha=0.2, label='training region')\n",
    "ax21.add_patch(rect)\n",
    "rect = Rectangle((0, -0.4), width=t_train, height=0.8, hatch='/', color='grey', alpha=0.2, label='training region')\n",
    "ax22.add_patch(rect)\n",
    "rect = Rectangle((0, -0.4), width=t_train, height=0.8, hatch='/', color='grey', alpha=0.2, label='training region')\n",
    "ax23.add_patch(rect)\n",
    "\n",
    "x_pos_all \t= np.array([4, 5, 6, 7, 8, 9, 10])\n",
    "labels \t\t= np.array([4, 5, 6, 7, 8, 9, 10])\n",
    "ax11.set_xticks(x_pos_all)\n",
    "ax11.set_xticklabels(labels)\n",
    "ax21.set_xticks(x_pos_all)\n",
    "ax21.set_xticklabels(labels)\n",
    "\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
